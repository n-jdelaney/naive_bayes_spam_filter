{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SMS Spam Filter Project\n",
    "## Creating a SMS spam filter using the multinomial Naive Bayes algorithm\n",
    "\n",
    "Text (SMS) message spam is a serious issue for millions of consumers around the world. While most messages are simply annoying, many spam messages are targeted campaings to steal consumers information. Filtering messages is not a simple endeavour. How does a computer decide whether an imcoming message is spam or legitimate? One method, the choice for this project, is to use the multinomial Naive Bayes algorithm. In general, the algorithm works as such:\n",
    "1. Learns how humans classify messages.\n",
    "2. Uses that human knowledge to estimate probabilities for new messages (using Bayes Theorem) as spam or not spam.\n",
    "3. Classifies a new message based on these probabilities. If the probability a message is spam is higher than non-spam, it will classify the message as spam, and vice-versa. If the probabilities are equal, then we require a human to decide the message classification.\n",
    "\n",
    "Our goal for this project is:\n",
    "* To build a SMS spam filter with at least an 80% accuracy using the multinomial Naive Bayes algorithm. \n",
    "\n",
    "We will use a dataset of 5,572 already classified SMS messages from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) to \"teach\" the computer how to classify messages. \n",
    "\n",
    "To begin our project, we will import the dataset and explore to familiarize ourselves with these data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and dataset\n",
    "import pandas as pd\n",
    "\n",
    "sms = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   5572 non-null   object\n",
      " 1   SMS     5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "sms.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find percentage of spam vs ham (non-spam) messages\n",
    "sms['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing a training and testing set\n",
    "On a simple exploration of our dataset, we see that about 87% of these messages are ham (non-spam) and 13% are spam. Next, we want to begin building our spam filter. However, before we start creating the software, we want to establish a method to test and verify the software works correctly. If we wait until the end of the project, we might be tempted to create a biased test so the software passes. \n",
    "\n",
    "When we complete the spam filter, we need to test how well it classifies new messages. In order to do this, we will first split our data into two sets:\n",
    "* A training set: we will use this to \"train\" the algorithm how to classify messages\n",
    "* A test set: we will use this to test the accuracy of the filter. \n",
    "\n",
    "In general, we want to use as much data as possible to train the algorithm while having enough data to test the algorithm. We will keep 80% of the dataset for training and 20% for testing. \n",
    "* The training set will have 4,458 messages\n",
    "* The test set will have 1,114 messages\n",
    "\n",
    "The test is simple: as the messages are already classified by a human, we only need to compare the classifications the algorithm makes to the human classifications. We will use this test once we actually build the software, but first we will split the dataset and begin creating the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will randomize the dataset\n",
    "random_sms = sms.sample(frac=1, random_state=1) # random_stat=1 for consistency\n",
    "\n",
    "# Split dataset into train and test\n",
    "train = random_sms.iloc[:4458, :].reset_index()\n",
    "test = random_sms.iloc[4458:, :].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.86541\n",
       "spam    0.13459\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify sample is consisten with entire dataset\n",
    "train['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.868043\n",
       "spam    0.131957\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify sample is consisten with entire dataset\n",
    "test['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Before we can move on to creating the algorithm, we need to clean these datasets. The goal for this cleaning process is to transform the train and test dataframes into dataframes with columns for each word in the entire vocabulary of words in these messages. Each row will still represent a message, but instead of an `SMS` column containing a string with the message, it will contain the number of times that word occurs in a given message. In other words, each column will contan the frequency for that word in the message. \n",
    "\n",
    "The SMS messages contain capitalizations and punctuation marks that we do not want. In order to transform the dataframes, we will first remove punctuation and make all the messages lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations from train\n",
    "train['SMS'] = train['SMS'].str.replace('\\W', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lowercase train\n",
    "train['SMS'] = train['SMS'].str.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a list called `vocabulary` that contains all the unique words that occur in these messages. We will first split the strings in `SMS` into lists. Then, we will add all the individual words to the list `vocabulary`. Finally, we will conver the list to a set and back to remove duplicate words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each row in `SMS` into a list\n",
    "train['SMS'] = train[\"SMS\"].str.split()\n",
    "\n",
    "# Initialize list for vocabulary\n",
    "vocabulary = []\n",
    "\n",
    "# Iterate over `SMS` and add each word to `vocabulary`\n",
    "for msg in train['SMS']:\n",
    "    for word in msg:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "# Keep only unique words\n",
    "vocabulary = set(vocabulary)\n",
    "vocabulary = list(vocabulary)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create a dictionary we can use to create a new dataframe with the columns representing all words in `vocabulary` and the rows indicating the number of times each word occurs in an individual message. \n",
    "\n",
    "* First, we will start by creating a dictionary `word_counts_per_sms` where each key is a unique word in `vocabulary` and each index is a list of zeros equal in length to the length of the training set, `train['SMS']`. \n",
    "* Next, we will loop over `train['SMS']` using the `enumerate()` function to get both the index and the SMS message. \n",
    "    * Using a nested loop, we loop over `sms` (where `sms` is a list of strings, where each sstring represents a word in a message).\n",
    "        * Inside that loop, we increment `word_counts_per_sms[word][index]` by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary word_counts_per_sms with each key representing a word from vocabulary and each index a list of 0s of len(train['SMS'])\n",
    "word_counts_per_sms = {\n",
    "    unique_word: [0] * len(train[\"SMS\"]) for unique_word in vocabulary\n",
    "}\n",
    "\n",
    "# Iterate over train[\"SMS\"] using enumerate() to add counts to each key in word_counts_per_sms\n",
    "for index, sms in enumerate(train['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform word_counts_per_sms to dataframe\n",
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "\n",
    "# Concatenate word_counts with train\n",
    "train_updated = pd.concat([train, word_counts], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Spam Filter\n",
    "Now, that we have a training set to work with, we can begin building the SMS Spam filter. The Naive Bayes Algorithm needs to calculate the probabilities of a given message being spam or not. Bayes Theorem gives us the following fomulae for probability:\n",
    "\n",
    "$$\n",
    "P(Spam|w_{1}, w_{2}, ..., w_{n}) \\space \\alpha \\space P(Spam) \\space • \\space \\prod_{i=1}^{n} P(w_{i}|Spam)\n",
    "$$(eq1)\n",
    "\n",
    "$$\n",
    "P(Ham|w_{1}, w_{2}, ..., w_{n}) \\space \\alpha \\space P(Ham) \\space • \\space \\prod_{i=1}^{n} P(w_{i}|Ham)\n",
    "$$(eq2)\n",
    "where  $w_{1}, w_{2}, ..., w_{n}$ is a given word in an incoming message. To calculate $P(w_{i}|Spam)$ and $P(w_{i}|Ham)$, we need to use the equations:\n",
    "\n",
    "$$\n",
    "P(w_{i}|Spam) = \\frac{N_{w_{i}|Spam} + \\alpha}{N_{Spam} + \\alpha \\space • \\space N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(w_{i}|Ham) = \\frac{N_{w_{i}|Ham} + \\alpha}{N_{Ham} + \\alpha \\space • \\space N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "First, we will calculate our constants for the above equations:\n",
    "* P(Spam) and P(Ham)\n",
    "* $N_{Spam}$, $N_{Ham}$, and $N_{Vocabulary}$\n",
    "\n",
    "We will keep in mind that:\n",
    "* $N_{Spam}$ is the number of words in all spam messages. It is not the number of spam messages, or the number of unique words in spam messages.\n",
    "* $N_{Ham}$ is the number of words in all non-spam messages. It is not the number of non-spam messages, or the number of unique words in non-spam messages.\n",
    "\n",
    "Furthermore, we will use Laplace Smoothing and set $\\alpha = 1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p_spam, p_ham, n_vocabulary, n_spam, n_ham\n",
    "p_spam = train_updated['Label'].value_counts(normalize=True)['spam'] # Probability of message being spam\n",
    "p_ham = train_updated['Label'].value_counts(normalize=True)['ham'] # Probability of message being non-spam\n",
    "\n",
    "n_vocabulary = len(vocabulary) # Number of unique words in the vocabulary\n",
    "n_ham = train_updated[train_updated['Label'] == 'ham'].iloc[:, 3:].sum().sum() # Number of words in non-spam messages\n",
    "n_spam = train_updated[train_updated['Label'] == 'spam'].iloc[:, 3:].sum().sum() # Number of words in spam messages\n",
    "\n",
    "alpha = 1 # Smoothing parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated the constant values needed for the algorithm, we will begin calculated probabilities of individual words in our vocabulary. In order to calculate the probabilities for incoming messages, we need to calculate $P(w_{i}|Spam)$ and $P(w_{i}|Ham)$, which will vary for each individual word. \n",
    "\n",
    "While $P(w_{i}|Spam)$ and $P(w_{i}|Ham)$ are different for each word, the probability for each individual word is constant. We need only to calculate $P(w_{i}|Spam)$ and $P(w_{i}|Ham)$ once for every word in our vocabulary. Then, the resulting probabilities can be used to classify messages. Because of this, the Naive Bayes Algorithm is very fast. Only a small number of calculations need to be performed for any new message. \n",
    "\n",
    "Next, we will create two dictionaries where each key is a unique word from `vocabulary` and each value is $P(w_{i}|Spam)$ or $P(w_{i}|Ham)$ for the given word. We will use the formiulae:\n",
    "$$\n",
    "P(w_{i}|Spam) = \\frac{N_{w_{i}|Spam} + \\alpha}{N_{Spam} + \\alpha \\space • \\space N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(w_{i}|Ham) = \\frac{N_{w_{i}|Ham} + \\alpha}{N_{Ham} + \\alpha \\space • \\space N_{Vocabulary}}\n",
    "$$\n",
    "to calculate the probabilities for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for P(w_i|Spam)\n",
    "p_w_i_spam = {\n",
    "    unique_word: 0 for unique_word in vocabulary\n",
    "}\n",
    "\n",
    "# Dictionary for P(w_i|Ham)\n",
    "p_w_i_ham = {\n",
    "    unique_word: 0 for unique_word in vocabulary\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate Spam and Ham messages\n",
    "spam_messages = train_updated.query(\"Label == 'spam'\")\n",
    "ham_messages = train_updated.query(\"Label == 'ham'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities for each word in vocabulary\n",
    "\n",
    "for word in vocabulary:\n",
    "    n_w_spam = spam_messages[word].sum()\n",
    "    n_w_ham = ham_messages[word].sum()\n",
    "    p_w_spam = (n_w_spam + alpha) / (n_spam + (alpha * n_vocabulary))\n",
    "    p_w_ham = (n_w_ham + alpha) / (n_ham + (alpha * n_vocabulary))\n",
    "    p_w_i_spam[word] = p_w_spam\n",
    "    p_w_i_ham[word] = p_w_ham\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the probabilities we need, we will create a function `classify()` that takes a message as input and returns the classification as `spam` or `ham`. We will use the formulae\n",
    "$$\n",
    "P(Spam|w_{1}, w_{2}, ..., w_{n}) \\space \\alpha \\space P(Spam) \\space • \\space \\prod_{i=1}^{n} P(w_{i}|Spam)\n",
    "$$(eq1)\n",
    "\n",
    "$$\n",
    "P(Ham|w_{1}, w_{2}, ..., w_{n}) \\space \\alpha \\space P(Ham) \\space • \\space \\prod_{i=1}^{n} P(w_{i}|Ham)\n",
    "$$(eq2)\n",
    "where  $w_{1}, w_{2}, ..., w_{n}$ is a given word in an incoming message\n",
    "to calculate the probabilities for each message. If $P(Spam|w_{1}, w_{2}, ..., w_{n})$ is higher than $P(Ham|w_{1}, w_{2}, ..., w_{n})$ then the message is classified as `spam`. Otherwise, if the opposite is true, the message will be classified as `ham`. If the probabilities are equivalent, a human is required to classify the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classify(message)\n",
    "import re\n",
    "\n",
    "def classify(message):\n",
    "    # Input message assumed to be a string.\n",
    "    # Strip punctuation and make lowercase. Then, split into list of strings.\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    # Initialize probabilities with p_spam and p_ham (In accordance with Bayes Theorem)\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    # Iterate over message and calculate probabilities of spam and ham\n",
    "    for word in message:\n",
    "        if word in p_w_i_spam:\n",
    "            p_spam_given_message *= p_w_i_spam[word]\n",
    "        if word in p_w_i_ham:\n",
    "            p_ham_given_message *= p_w_i_ham[word]\n",
    "\n",
    "\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.3481290211300841e-25\n",
      "P(Ham|message): 1.9368049028589875e-27\n",
      "Label: Spam\n",
      "P(Spam|message): 2.4372375665888117e-25\n",
      "P(Ham|message): 3.687530435009238e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')\n",
    "classify(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the algorithm\n",
    "Now that we have completed the `classify()` function, we can test it on the `test` DataFrame we split from the original dataset. We will compare the output of our algorithm with the human classifications in the `test` set to verify the accuracy of our work. \n",
    "\n",
    "First, we will modify the `classify()` function to return `ham`, `spam`, or `needs human classification` instead of printing the results. Then we will create a new column in the `test` DataFrame and apply the function to the `SMS` column. We will then calculate the accuracy of the algorithm:\n",
    "$$\n",
    "Accuracy \\space = \\space \\frac{number \\space of \\space correctly \\space classified \\space messages}{total \\space number \\space of \\space classified \\space messages}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new function classify_test_set(message)\n",
    "\n",
    "def classify_test_set(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in p_w_i_spam:\n",
    "            p_spam_given_message *= p_w_i_spam[word]\n",
    "\n",
    "        if word in p_w_i_ham:\n",
    "            p_ham_given_message *= p_w_i_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2131</td>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3418</td>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3424</td>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1538</td>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5393</td>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index Label                                                SMS predicted\n",
       "0   2131   ham          Later i guess. I needa do mcat study too.       ham\n",
       "1   3418   ham             But i haf enuff space got like 4 mb...       ham\n",
       "2   3424  spam  Had your mobile 10 mths? Update to latest Oran...      spam\n",
       "3   1538   ham  All sounds good. Fingers . Makes it difficult ...       ham\n",
       "4   5393   ham  All done, all handed in. Don't know if mega sh...       ham"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create predicted column in test\n",
    "test['predicted'] = test['SMS'].apply(classify_test_set)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of filter is:  0.9874326750448833\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "test['correct'] = test['Label'] == test['predicted']\n",
    "print('Accuracy of filter is: ', (test['correct'].sum() / test.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our spam filter has performed very well, having a 98.7% accuracy for classifying 1,114 new messages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
