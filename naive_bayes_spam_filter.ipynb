{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SMS Spam Filter Project\n",
    "## Creating a SMS spam filter using the multinomial Naive Bayes algorithm\n",
    "\n",
    "Text (SMS) message spam is a serious issue for millions of consumers around the world. While most messages are simply annoying, many spam messages are targeted campaings to steal consumers information. Filtering messages is not a simple endeavour. How does a computer decide whether an imcoming message is spam or legitimate? One method, the choice for this project, is to use the multinomial Naive Bayes algorithm. In general, the algorithm works as such:\n",
    "1. Learns how humans classify messages.\n",
    "2. Uses that human knowledge to estimate probabilities for new messages (using Bayes Theorem) as spam or not spam.\n",
    "3. Classifies a new message based on these probabilities. If the probability a message is spam is higher than non-spam, it will classify the message as spam, and vice-versa. If the probabilities are equal, then we require a human to decide the message classification.\n",
    "\n",
    "Our goal for this project is:\n",
    "* To build a SMS spam filter with at least an 80% accuracy using the multinomial Naive Bayes algorithm. \n",
    "\n",
    "We will use a dataset of 5,572 already classified SMS messages from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) to \"teach\" the computer how to classify messages. \n",
    "\n",
    "To begin our project, we will import the dataset and explore to familiarize ourselves with these data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and dataset\n",
    "import pandas as pd\n",
    "\n",
    "sms = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   5572 non-null   object\n",
      " 1   SMS     5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "sms.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find percentage of spam vs ham (non-spam) messages\n",
    "sms['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing a training and testing set\n",
    "On a simple exploration of our dataset, we see that about 87% of these messages are ham (non-spam) and 13% are spam. Next, we want to begin building our spam filter. However, before we start creating the software, we want to establish a method to test and verify the software works correctly. If we wait until the end of the project, we might be tempted to create a biased test so the software passes. \n",
    "\n",
    "When we complete the spam filter, we need to test how well it classifies new messages. In order to do this, we will first split our data into two sets:\n",
    "* A training set: we will use this to \"train\" the algorithm how to classify messages\n",
    "* A test set: we will use this to test the accuracy of the filter. \n",
    "\n",
    "In general, we want to use as much data as possible to train the algorithm while having enough data to test the algorithm. We will keep 80% of the dataset for training and 20% for testing. \n",
    "* The training set will have 4,458 messages\n",
    "* The test set will have 1,114 messages\n",
    "\n",
    "The test is simple: as the messages are already classified by a human, we only need to compare the classifications the algorithm makes to the human classifications. We will use this test once we actually build the software, but first we will split the dataset and begin creating the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will randomize the dataset\n",
    "random_sms = sms.sample(frac=1, random_state=1) # random_stat=1 for consistency\n",
    "\n",
    "# Split dataset into train and test\n",
    "train = random_sms.iloc[:4458, :].reset_index()\n",
    "test = random_sms.iloc[4459:, :].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.86541\n",
       "spam    0.13459\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify sample is consisten with entire dataset\n",
    "train['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.867925\n",
       "spam    0.132075\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify sample is consisten with entire dataset\n",
    "test['Label'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Before we can move on to creating the algorithm, we need to clean these datasets. The goal for this cleaning process is to transform the train and test dataframes into dataframes with columns for each word in the entire vocabulary of words in these messages. Each row will still represent a message, but instead of an `SMS` column containing a string with the message, it will contain the number of times that word occurs in a given message. In other words, each column will contan the frequency for that word in the message. \n",
    "\n",
    "The SMS messages contain capitalizations and punctuation marks that we do not want. In order to transform the dataframes, we will first remove punctuation and make all the messages lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations from train\n",
    "train['SMS'] = train['SMS'].str.replace('\\W', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lowercase train\n",
    "train['SMS'] = train['SMS'].str.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a list called `vocabulary` that contains all the unique words that occur in these messages. We will first split the strings in `SMS` into lists. Then, we will add all the individual words to the list `vocabulary`. Finally, we will conver the list to a set and back to remove duplicate words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each row in `SMS` into a list\n",
    "train['SMS'] = train[\"SMS\"].str.split()\n",
    "\n",
    "# Initialize list for vocabulary\n",
    "vocabulary = []\n",
    "\n",
    "# Iterate over `SMS` and add each word to `vocabulary`\n",
    "for msg in train['SMS']:\n",
    "    for word in msg:\n",
    "        vocabulary.append(word)\n",
    "\n",
    "# Keep only unique words\n",
    "vocabulary = set(vocabulary)\n",
    "vocabulary = list(vocabulary)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create a dictionary we can use to create a new dataframe with the columns representing all words in `vocabulary` and the rows indicating the number of times each word occurs in an individual message. \n",
    "\n",
    "* First, we will start by creating a dictionary `word_counts_per_sms` where each key is a unique word in `vocabulary` and each index is a list of zeros equal in length to the length of the training set, `train['SMS']`. \n",
    "* Next, we will loop over `train['SMS']` using the `enumerate()` function to get both the index and the SMS message. \n",
    "    * Using a nested loop, we loop over `sms` (where `sms` is a list of strings, where each sstring represents a word in a message).\n",
    "        * Inside that loop, we increment `word_counts_per_sms[word][index]` by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary word_counts_per_sms with each key representing a word from vocabulary and each index a list of 0s of len(train['SMS'])\n",
    "word_counts_per_sms = {\n",
    "    unique_word: [0] * len(train[\"SMS\"]) for unique_word in vocabulary\n",
    "}\n",
    "\n",
    "# Iterate over train[\"SMS\"] using enumerate() to add counts to each key in word_counts_per_sms\n",
    "for index, sms in enumerate(train['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform word_counts_per_sms to dataframe\n",
    "word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "\n",
    "# Concatenate word_counts with train\n",
    "train_updated = pd.concat([train, word_counts], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
